决策树
    构造：生成一棵完整的决策树，即选择什么属性作为决策树的节点的过程
        根节点：最顶端的属性节点，也即父节点
        内部节点：即中间的节点，都叫内部节点，内部节点可以有子节点
        叶节点：最下面的节点，不会再有其他节点产生，即决策结果

        构造时要解决的问题：
            选择什么属性作为根节点
            选择什么属性作为子节点
            什么时候终止并得到目标状态，即叶节点
    剪枝：剪掉不必要的子节点，防止过拟合。剪枝是为了在减少判断的情况下也能够得到不错的或者更好的结果。
        过拟合overfitting：模型训练的‘太好’，在实际运用过程中会产生死板的情况，导致分类错误。
            造成过拟合的原因之一是因为训练集中样本量较小，如果决策树属性太多，构造出的决策树能够完美的将训练集中的样本进行分类，如此，它会将训练集中的数据特点当成所有数据的特点，导致在给真实数据分类时产生错误，也就是‘泛化能力’差。
                泛化能力：分类器在训练过程中通过训练集抽象出来的分类能力，过于依赖训练集的数据，得到的决策树会导致容错率低，即泛化能力差。
        预剪枝Pre-Pruning：构造的过程中进行剪枝，方法是在构造过程中对节点评估，如果对某个节点进行划分不能带来准确性的提升，划分就毫无意义，那么就将其当成叶节点。
        后剪枝Post-Pruning：生成完决策树之后进行剪枝，方法是从叶节点开始，逐级向上对每个节点进行评估，如果剪掉某个节点子树对结果影响不大，或能够带来准确性的提升，那么就剪掉该节点子树，方法是用这个节点子树的叶节点来替代该节点。
    
    纯度：可以将决策树的构造过程看成是寻找纯净划分的过程，在数学中用纯度来表示，纯度也就是让目标变量分歧最小。
        假设有3个集合如下：
        集合1：6次都去打篮球
        集合2：4次去打篮球，2次不去
        集合3：3次去打篮球，3次不去
        那么按照纯度指标，集合1 > 集合2 > 集合3，集合1的分歧最小，集合3分歧最大
    信息熵entropy：信息的不确定度。在信息论中，随机离散事件出现的概率具有不确定性。当不确定性越大，所包含的信息量越大，信息熵就越高。
        单个符号的信息熵：单个符号在整个信息中的概率为p，信息熵 = -p*log2(p)，所以符号信息熵求和即可。
        计算之后发现信息熵越高，纯度越低。
    构造决策树时基于纯度构建，经典的‘不纯度’指标有三种：
        信息增益（ID3算法）：划分可以带来纯度的提高，信息熵的下降。
            计算方法：
                1. 计算根节点的信息熵
                2. 计算每个子节点的信息熵
                3. 计算每个子节点的归一化信息熵，即子节点占父节点的比例
                4. 根节点的信息熵 -  所有子节点归一化信息熵的和
                5. 选择信息增益最大的那个属性作为节点
            缺陷：倾向于选择取值较多的属性，因此有一定的概率会发生将对分类任务作用不大的属性选为最优属性。
        信息增益率（C4.5算法）：针对ID3算法的改进
            计算方法:
                采用信息增益率：信息增益率 = 信息增益 / 属性熵（属性熵和信息熵计算方法一样，只不过这里是属性的概率）
            采用悲观剪枝（PEP）：后剪枝技术的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否剪枝。该方法不再需要一个单独的测试数据集
            离散化处理连续属性：可以对连续属性进行离散化处理，比如湿度不按‘高’‘中’‘低’来分，而是取不同的湿度值，此时，C4.5选择具有最高信息增益的划分所对应的阈值
            处理缺失值：如果有缺失值，会将计算的信息增益率乘以该节点样本的权重。比如样本数为7，但温度下的数据只有6，缺失一条数据，那么温度作为节点的信息增益率要乘以它的权重即（6/7）作为最终的信息增益率
        基尼指数（cart算法）：分类回归树，只支持二叉树
            分类树：处理离散型数据，输出的是样本的类别
            回归树：可以对连续型的数值进行预测，输出的是一个数值
            基尼系数：衡量一个国家的财富差距的常用指标，大于0.4表示财富差异悬殊，0.2~0.4之间表明分配合理，财富差距不大。因其本身反应了样本的不确定性，当基尼系数越小，样本之间的差异越小，不确定度越低，因此在cart算法里用它来作为指标，在构造分类树时，会选择基尼系数最小的属性作为属性的划分。
            分类树计算方法：
                1. 节点t属于某类别的概率是p
                2. 节点t的基尼系数 = 1 - 各类别概率（即p）的平方和
                3. 父节点的基尼系数 = 各节点t的归一化基尼系数之和
            回归树中，采用样本的离散程度来评价‘不纯度’。通常采用方差，对应到sklearn中的目标函数也就是最小二乘偏差(LSD)的方法来找到节点的划分。还可以用最小绝对偏差(LAD)的方法，即样本个体到样本均值的差值，取 |样本个体 - 样本均值| 的绝对值。
            cart决策树的剪枝：
                采用CCP方法，cost-complexity prune，是一种后剪枝方法，中文叫做代价复杂度。该方法使用的指标是：节点的表面误差率增益值
                计算方法：
                    1. 假设以t作为节点，计算剪枝前以t为节点的子树的误差
                    2. 计算剪枝后t节点的误差
                    3. 剪枝后的叶子数为剪枝前以t为节点的子树的叶子数 - 1
                    4. 节点的表面误差率增益值 =（剪枝后t节点的误差 - 剪枝前以t为节点的子树的误差）/ 剪枝后的叶子数
                    
                我们希望剪枝前后误差最小，所以要寻找的是最小增益值所对应的节点，把它剪掉。这时候生成了第一个子树。重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。（这步获得了子树序列）
                得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。（通过验证集，在获得的子树序列中找到最优树）
                
    ID3算法举例
        有如下数据训练集：
        天气    温度    湿度    刮风    是否打篮球
        晴天    高      中      否      否
        晴天    高      中      是      否
        阴天    高      高      否      是
        小雨    高      高      否      是
        小雨    低      高      否      否
        晴天    中      中      是      是
        阴天    中      高      是      否
        
        ID3算法如何构造决策树：
            根据计算方法一步步来
            1. 计算根节点信息熵
                数据集样本数为7，其中打篮球3次，不打篮球4次：-(3/7)*log2(3/7)-(4/7)*log2(4/7) = 0.985
            2. 计算每个子节点信息熵
                天气为节点，属性有晴天，阴天，小雨。
                    晴天时打篮球情况为：否，否，是；对应信息熵为：-(2/3)*log2(2/3)-(1/3)*log2(1/3) = 0.918
                    阴天时打篮球情况为：是，否；对应信息熵为：-(1/2)*log2(1/2)-(1/2)*log2(1/2) = 1
                    小雨时打篮球情况为：是，否；对应信息熵为：-(1/2)*log2(1/2)-(1/2)*log2(1/2) = 1
                归一化信息熵为：3/7 * 0.918 + 2/7 * 1 + 2/7 * 1 = 0.965

                温度为节点，属性有高，中，低。
                    高时打篮球情况为：否，否，是，是；对应信息熵为：-(2/4)*log2(2/4)-(2/4)*log2(2/4) = 1
                    中时打篮球情况为：是，否；对应信息熵为：-(1/2)*log2(1/2)-(1/2)*log2(1/2) = 1
                    低时打篮球情况为：否；对应信息熵为：-log2(1) = 0
                归一化信息熵为：4/7 * 1 + 2/7 * 1 + 0 = 0.857

                湿度为节点，属性有高，中。
                    高时打篮球情况为：是，是，否，否；对应信息熵为：-(2/4)*log2(2/4)-(2/4)*log2(2/4) = 1
                    中时打篮球情况为：否，否，是；对应信息熵为：-(2/3)*log2(2/3)-(1/3)*log2(1/3) = 0.918
                归一化信息熵为：4/7 * 1 + 3/7 * 0.918 = 0.965

                刮风为节点，属性有是，否。
                    是时打篮球情况为：否，是，否；对应信息熵为：-(2/3)*log2(2/3)-(1/3)*log2(1/3) = 0.918
                    否时打篮球情况为：否，是，是，否；对应信息熵为：-(2/4)*log2(2/4)-(2/4)*log2(2/4) = 1
                归一化信息熵为：3/7 * 0.918 + 4/7 * 1 = 0.965
            3. 计算每个节点的信息增益
                天气 信息增益为：0.985 - 0.965 = 0.020
                温度 信息增益为：0.985 - 0.857 = 0.128
                湿度 信息增益为：0.985 - 0.965 = 0.020
                刮风 信息增益为：0.985 - 0.965 = 0.020
            4. 选择温度作为第一个节点，即根节点

            第一个节点选定，属性有高、中、低，即分三个叉，每个叉下面又应该选择什么作为节点呢？
            1. 首先来算温度（高）下面应该选择什么来作为节点，温度为高，打篮球情况为：否，否，是，是；信息熵为：-(2/4)*log2(2/4)-(2/4)*log2(2/4) = 1
            2. 同理计算子节点信息熵，不过此时子节点的打篮球行为对应的是温度为高时的情况
                天气为子结点，对应的温度为高时打篮球行为：
                    晴天：否，否；信息熵：-log2(1) = 0
                    阴天：是；信息熵：-log2(1) = 0
                    小雨：是；信息熵：-log2(1) = 0
                归一化信息熵：0

                湿度为子节点，对应的温度为高时打篮球行为：
                    湿度为中：否，否；信息熵：-log2(1) = 0
                    湿度为高：是，是；信息熵：-log2(1) = 0
                归一化信息熵：0

                刮风为子节点，对应的温度为高时打篮球行为：
                    刮风为否：否，是，是；信息熵：-(2/3)*log2(2/3)-(1/3)*log2(1/3) = 0.918
                    刮风为是：否；信息熵：-log2(1) = 0
                归一化信息熵：3/4 * 0.918 + 0 = 0.6885
            3. 计算每个节点的信息增益
                天气：1 - 0 = 1
                湿度：1 - 0 = 1
                刮风：1 - 0.689 = 0.3115
            4. 选择信息增益大的作为子节点，此处可选择天气或湿度。
            5. 其他节点可以同理计算得出

            





